% !TeX document-id = {e7fef4b9-afbc-4e81-94aa-c4847183d047}
% !BIB program = biber
\documentclass[conference, letterpaper, 10pt, times]{IEEEtran}
\usepackage{nopageno}

%\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}

\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, calc, fit, positioning}

\usepackage{etoolbox}
\usepackage[binary-units, per-mode=symbol]{siunitx}
\robustify\bfseries
\sisetup{detect-all, range-phrase=--, range-units=single, detect-weight=true,detect-inline-weight=math}

\makeatletter
\let\MYcaption\@makecaption
\makeatother

\usepackage[font=footnotesize]{subcaption}

\makeatletter
\let\@makecaption\MYcaption
\makeatother

\usepackage[basic]{complexity}
\usepackage[super,negative]{nth}

\usepackage[british]{babel}
\usepackage{csquotes}

\usepackage{booktabs}
\usepackage[
activate={true,nocompatibility},
final,
tracking=true,
%kerning=true,spacing=true
]{microtype}
\microtypecontext{spacing=nonfrench}

%% Fix indent in new section...
\newcommand{\subparagraph}{}
\usepackage{titlesec}
\titlespacing*{\section}{0pt}{1.5ex}{0.7ex}
\titlespacing*{\subsection}{0pt}{1.5ex}{0.7ex}
\titlespacing*{\paragraph}{0pt}{1.5ex}{0.7ex}
%\titleformat*{\filcenter\scshape}

\usepackage{enumitem}
\setlist[description]{leftmargin=8em,style=nextline}

%bib
\usepackage[style=ieee,maxnames=3,mincitenames=1,maxcitenames=2,maxbibnames=99,
mincrossrefs=99,minxrefs=99,
sortcites,
%backend=bibtex,
uniquelist=false]{biblatex}
\addbibresource{papers-off.bib}
\addbibresource{confs-off.bib}
\addbibresource{books-off.bib}
\addbibresource{rfc.bib}
\addbibresource{misc.bib}

%picky abt et al.
%\usepackage{xpatch}

%\xpatchbibmacro{name:andothers}{%
%	\bibstring{andothers}%
%}{%
%	\bibstring[\emph]{andothers}%
%}{}{}

% emph'd et al. for ieee style
\DefineBibliographyStrings{english}{%
	andothers = {\emph{et al}\adddot}
}
\DeclareFieldFormat[inproceedings]{url}{}
\DeclareFieldFormat[article]{url}{}

%opening!

%\newcommand{\mytitle}{Improving Direct-Control Reinforcement Learning for Network Intrusion Prevention}
\newcommand{\mytitle}{Improving Direct-Control Reinforcement Learning for Network Intrusion Prevention}

\usepackage{url}
\usepackage{hyperref}
\usepackage{cleveref}
\newcommand{\crefrangeconjunction}{--}

\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black,
	pdftitle={\mytitle{}},
%	pdfauthor={Kyle A. Simpson, Dimitrios P. Pezaros}
	pdfauthor={Anonymous Giraffe and Badger}
}
\newcommand*{\email}[1]{\href{mailto:#1}{\nolinkurl{#1}} } 

\usepackage{titling}
%\settowidth{\thanksmarkwidth}{*}
%\setlength{\thanksmargin}{-\thanksmarkwidth}
%
%%% Enable /thanks
%\IEEEoverridecommandlockouts
%\makeatletter
%\def\footnoterule{\relax%
%	\kern-5pt
%	\hbox to \columnwidth{\hfill\vrule width 0.5\columnwidth height 0.4pt\hfill}
%	\kern4.6pt}
%\makeatother

% make math easy
\newcommand{\acval}[3]{\ensuremath{\operatorname{\hat{q}}(#1, #2, #3)}}
\newcommand{\wvec}[1]{\ensuremath{\bm{w}_{#1}}}

%thm environments
\newtheorem{thm}{Theorem}
\newtheorem{corr}{Corollary}[thm]

\newcommand{\fakepara}[1]{\noindent\textbf{#1:}}

\makeatletter\let\expandableinput\@@input\makeatother

\makeatletter
\DeclareRobustCommand{\rvdots}{%
	\vbox{
		\baselineskip4\p@\lineskiplimit\z@
		\kern-\p@
		\hbox{.}\hbox{.}\hbox{.}
}}
\makeatother

% Official colours!

\definecolor{uofguniversityblue}{rgb}{0, 0.219608, 0.396078}

\definecolor{uofgheather}{rgb}{0.356863, 0.32549, 0.490196}
\definecolor{uofgaquamarine}{rgb}{0.603922, 0.72549, 0.678431}
\definecolor{uofgslate}{rgb}{0.309804, 0.34902, 0.380392}
\definecolor{uofgrose}{rgb}{0.823529, 0.470588, 0.709804}
\definecolor{uofgmocha}{rgb}{0.709804, 0.564706, 0.47451}

\definecolor{uofglawn}{rgb}{0.517647, 0.741176, 0}
\definecolor{uofgcobalt}{rgb}{0, 0.615686, 0.92549}
\definecolor{uofgturquoise}{rgb}{0, 0.709804, 0.819608}
\definecolor{uofgsunshine}{rgb}{1.0, 0.862745, 0.211765}
\definecolor{uofgpumpkin}{rgb}{1.0, 0.72549, 0.282353}
\definecolor{uofgthistle}{rgb}{0.584314, 0.070588, 0.447059}
\definecolor{uofgpillarbox}{rgb}{0.701961, 0.047059, 0}
\definecolor{uofglavendar}{rgb}{0.356863, 0.301961, 0.580392}

\definecolor{uofgsandstone}{rgb}{0.321569, 0.278431, 0.231373}
\definecolor{uofgforest}{rgb}{0, 0.317647, 0.2}
\definecolor{uofgburgundy}{rgb}{0.490196, 0.133333, 0.223529}
\definecolor{uofgrust}{rgb}{0.603922, 0.227451, 0.023529}

%-------------------------------------%
%-------------------------------------%

\title{\mytitle{}}
\author{
%	Kyle A.\ Simpson\thanks{This work was supported by the Engineering and Physical Sciences Research Council [grant number EP/M508056/1].},
%	Dimitrios P.\ Pezaros\\
%	University of Glasgow, Glasgow, Scotland,\\
%	\email{k.simpson.1@research.gla.ac.uk}
Anonymous Giraffe,
Anonymous Badger\\
Unnamed Department, Nowhere\\
\email{giraffe.a@unnamed.com}
}

% Remove date, leave no spacing.
\predate{}
\postdate{}
\date{}

\begin{document}

%% If needed, make urls typewritery
%\urlstyle{tt}

\maketitle

\begin{abstract}
Network intrusion detection and prevention systems backed by machine learning (and the autonomous operation they promise) have been long-heralded, but face problems hampering effective deployment.
The detection problem in this domain is fraught with difficulty; it is an evolving, non-stationary problem as usage patterns shift, new protocols and applications are introduced, and is compounded by burstiness and seasonal variation.

\emph{Reinforcement learning} (RL) may overcome the detection problem for certain classes of anomaly by managing and monitoring \emph{consequences}; an agent's role is to learn to optimise performance criteria (which are always available).

?? We explore the design space, and improve upon existing approaches.

We present...
?? Contribs

?? Taking up space to figure out how much room I have for an intro

?? still taking up space...

?? still going...

?? done...
\end{abstract}

\section{Introduction}

Network anomaly detection, intrusion detection and intrusion prevention are continually evolving problems, compounded by the partial, non-IID view of data at each point in the network.
Attacks and anomalous behaviours evolve, becoming more sophisticated or employing new vectors to harm a network or system's confidentiality, integrity, or availability without being detected.
These attacks and anomalies have measurable consequences and symptoms which allow a skilled analyst to infer new signatures for detection by misuse-based classifiers, but unseen attacks may only be defended against after-the-fact.
This issue is inherent to \emph{misuse-} or \emph{signature-based} intrusion detectors, and it has been long-hoped that \emph{anomaly-based} detectors would surpass this by making effective use of statistical measures.

While \emph{machine learning} (ML) approaches seem like a sensible fit for this problem, in \citeyear{DBLP:conf/sp/SommerP10} \textcite{DBLP:conf/sp/SommerP10} identified the `failure to launch' of ML-based anomaly detection systems---to quite a large extent, this remains the case today.
Their application is made difficult due to significant operational differences from standard ML tasks, alongside certain characteristics of network traffic.
Briefly, these are the diversity of network traffic across varying timescales \cite{DBLP:conf/sp/SommerP10} and significant burstiness \cite{DBLP:journals/ccr/LelandWTW95}.
Above the aggregate level, the constant deployment of new services and new protocols means that traffic is \emph{non-stationary} and displays an evolving notion of normality.
Further issues arise from the extraordinarily low tolerance for false positives inherent to network intrusion detection \cite{DBLP:conf/ccs/Axelsson99}, and the challenges encountered when learning from unlabelled (often partial) data.
All of these factors greatly inflate the difficulty of the detection problem.

For certain classes of problem like flooding-based DDoS attacks, \emph{reinforcement learning} (RL) offers another perspective.
The role of an RL agent differs from that of a standard classifier, adaptively reacting to threats by assuming the role of a feedback loop for network optimisation, typically to safeguard service guarantees.
In a sense, this allows us to ``overcome'' the difficulties of the detection problem by monitoring \emph{performance characteristics and consequences} in real-time; by looking for the effect rather than the cause.
The intent is to augment what existing misuse-based solutions can provide, by automatically alerting, recording and controlling what are believed to be illegal system states.
%Whether it takes direct control of the network, or is used indirectly to optimise a key part of another system, more powerful `deep' RL techniques (and well-founded action spaces) aren't yet well explored for network IDS/IPS.
%These range from more modern training algorithms \cite{DBLP:journals/corr/SchulmanWDRK17, DBLP:conf/icml/SchulmanLAJM15}, to evolutionary strategies \cite{DBLP:journals/corr/SalimansHCS17, DBLP:journals/corr/abs-1802-08842}, hierarchical action composition \cite{DBLP:journals/corr/abs-1710-09767}, and competitive multi-agent learning \cite{DBLP:journals/corr/abs-1710-03748}.

To date, there have been few applications of this class of algorithms towards intrusion prevention which make use of their full potential for online control, rather than using them as the basis of a classifier.
We aim to take steps to redress this and establish their proper capabilities, beyond simple ``blind application''.
In service of this goal, this paper contributes:
\begin{itemize}
	\item A DDoS mitigation system based on direct-control reinforcement learning designed for deployment in real-world software-defined networks (\cref{sec:environment-and-rl-algorithm,sec:rethinking-the-state-space}).
	\item Important weaknesses and flaws in the past design and evaluation of similar techniques \cite{DBLP:journals/eaai/MalialisK15}---a deconstruction and empirical study on their formulation, flaws, and risk factors concerning many traffic distributions (\cref{sec:performance-in-an-emulated-environment}).
	\item A reactive simulation of web-server traffic, designed to test system characteristics which packet trace playback fails to capture (\cref{sec:a-new-normal}).
	\item A source-level granularity approach to RL-driven DDoS prevention, improving upon past aggregate-based approaches (\cref{sec:rethinking-the-state-space}), alongside an empirical evaluation (\cref{sec:the-results-of-doing-so}).
\end{itemize}

\section{Background and Threat Model}
%?? Introduce RL, related definitions etc.
\subsection{Reinforcement Learning}
\emph{Reinforcement learning} (RL) is a variant of machine learning principally concerned with with training an agent to choose an optimal sequence of actions in the pursuit of a given task \cite{RL2E}.
We assume the agent has a certain amount of knowledge whenever a decision must be made: at any point in time $t$ it knows which \emph{state} it is in ($S_t \in \mathcal{S}$), the set of \emph{actions} which are available to it ($\operatorname{A}(S_t) \subseteq \mathcal{A}$) and, if available, a numeric \emph{reward} obtained from the last action chosen ($R_t \in \mathbb{R}, A_{t-1} \in \operatorname{A}(S_{t-1})$).
This model of system interaction is best described as a \emph{Markov decision process} (MDP).
RL methods combine this information with a current \emph{policy} $\pi$ to determine which action should be taken: such a choice need not be optimal, if an agent needs to further explore some region of the state space.
The policy is then further refined by updating value estimates for state-action pairs or via policy gradient methods, meaning that RL-based approaches learn adaptively and online if reward functions are available in the environment they are deployed in.
From any point in a sequence of decisions, we may describe the sum of rewards yet to come as the \emph{discounted return}, $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+2} + \ldots$, choosing the discount factor $\gamma \in [0,1)$ to determine how crucial future rewards are vis-\`{a}-vis the current state.
Formally, an agent's goal is to choose actions which maximise the \emph{expected discounted return} $\operatorname{\mathbb{E}_{\pi}}[G_t]$.

%?? Include some details of function approximation in the formalisation? I.e. tile coding, stability and convergence guarantees...

There is immense variation in \emph{how} policies and/or values may be learned, reliant upon the learning environment, problem and required convergence guarantees.
In particular, we shall focus on methods which choose actions according to their value estimates from the current state.
To cope with a continuous state- and/or action-space, one valuable technique is to employ \emph{tile-coding} \cite[pp.\ \numrange{217}{221}]{RL2E}---this converts our state from $\mathbb{R}^d \times \mathcal{A}$ into a sparse boolean feature vector $\operatorname{\mathbf{x}}(s, a)$ by subdividing it into a number of overlapping $(d+ \dim{\mathcal{A}})$-dimensional grids with an optional bias component.
The number of tilings has a direct effect on the resolution of the discretisation.
Entries of $\operatorname{\mathbf{x}}(s, a)$ are set to 1 if the state-action pair exists within the corresponding tile.
This transform, as an example of linear function approximation, then necessitates the use of \emph{1-step semi-gradient Sarsa} \cite[pp.\ \numrange{243}{244}]{RL2E}.
Given a weight vector $\wvec{0}=\bm{0}$ and learning rate $\alpha$, we may approximate an action $a$'s value in any state $s$ taking $\operatorname{q}(s, a) \approx \acval{s}{a}{\wvec{}}$ by continually updating $\wvec{t}$ as follows:
\begin{subequations}
	\begin{gather}
	\acval{s}{a}{\wvec{}} = \wvec{}^{\top} \operatorname{\mathbf{x}}(s, a),\\
	\delta_t = R_{t+1} + \gamma \acval{S_{t+1}}{A_{t+1}}{\wvec{t}} - \acval{S_t}{A_t}{\wvec{t}},\\
	\bm{w}_{t+1} = \bm{w}_{t} + \alpha \delta_t \nabla{\acval{S_t}{A_t}{\wvec{t}}},
	\end{gather}
	\label{eqn:sg-sarsa}
	taking $\nabla$ with respect to $\wvec{}$.
\end{subequations}

Computing the approximate value of every action available in the current state forms the basis of a policy.
Actions with maximal value can be chosen each time (the \emph{greedy} policy), we might modify this by taking random actions with probability $\epsilon$ to encourage early exploration (the \emph{$\epsilon$-greedy policy}), or we might use some other mechanism.

If $\operatorname{A}(s) = \mathcal{A}$ in every state (and actions are themselves discrete), these properties allow a particularly efficient (vectorised) implementation of the policy and update rules by storing a vector of action values for each state encountered.
Observing that $\nabla{\acval{s}{a}{\wvec{}}} = \operatorname{\mathbf{x}}(s, a)$, further optimisations arise by considering that a tile-coded feature vector is a binary vector of constant Hamming weight (and so is amenable to representation as an array of indices).
Action values for any state are then obtained by summation of the weight vectors from all activated tiles.
Moreover, we need not store action values for tiles which have not yet been visited, conserving memory.

%?? Is this \emph{actually} just sarsa? We're using fn approx (of course), but this is fraught with its own difficulties. Is it strictly speaking correct to describe it as Sarsa at this point? It's, at the very least, 1-step semi-gradient Sarsa given that it is clearly on-policy w/ fn approx...

%\subsection{Intrusion Detection}
%Probably want to talk about NIDS/IPS,
%?? Discuss mininet? Networking terms? SDN stuff?

\subsection{Distributed Denial of Service (DDoS)}
%?? DDoS attack variants (leading into characteristics, supporting features). Amplification (UDP \cite{DBLP:conf/ndss/Rossow14}, TCP \cite{DBLP:conf/uss/KuhrerHRH14}), Transit-link (Crossfire \cite{DBLP:conf/sp/KangLG13}, Coremelt \cite{DBLP:conf/esorics/StuderP09}). Mirai botnet's involvement \cite{DBLP:conf/uss/AntonakakisABBB17}.

%?? Explain amplification attack, maybe transit-link?

DDoS attacks are concentrated efforts by many hosts to reduce the availability of a service, typically to inflict financial harm or as an act of vandalism.
Attackers achieve this either by exploiting peculiarities of operating system or application behaviour (e.g., \emph{SYN flooding attacks}), or overwhelming their target through sheer volume of requests or inbound packets (\emph{flooding-based attacks}).
Hosts often participate unwillingly, typically having been recruited into a \emph{botnet} by malware infection to be orchestrated from elsewhere \cite{DBLP:conf/uss/AntonakakisABBB17}.

Although there are variations of each class of attack, flooding attacks are the most relevant to our work.
\emph{Amplification attacks} exploit the presence of services who eagerly send large replies in response to small requests, where UDP-based services like DNS and NTP are most exploitable \cite{DBLP:conf/ndss/Rossow14, DBLP:conf/uss/KuhrerHRH14}.
Malicious hosts send many small requests, spoofed to appear as though they originated from the victim, causing many large replies to be sent to the intended target---significantly increasing a botnet's throughput while masking the identity of each participant.
\emph{Transit-link/link-flooding attacks} have been the subject of recent attention, wherein malicious traffic is forwarded across core links needed to reach a target (but not to the target itself) \cite{DBLP:conf/sp/KangLG13, DBLP:conf/esorics/StuderP09}.

\subsection{Motivation}\label{sec:motivation}
%?? What makes RL a suitable method for network anomaly detection, what features are most relevant?
%?? Point I was thinking of: feedback-loop-like model allows monitoring \emph{after} an action is taken to (in theory) allow forgiveness of mistakenly punished flows. This does hinge on taking a flow-by-flow look at the state space, but if we can combine knowledge of current state (duh!), the last action taken (i.e. an indicator of our previous assessment [such as high pdrop $\implies$ bad]) then perhaps a flow which falls off identically to a legit flow can be rescued.
Moving beyond the overt benefits of choosing RL-based defences for coping with non-stationary problems, we believe that there are concrete reasons for their use here.
We have seen that for other domains in particular, misclassification is a serious problem, which can introduce \emph{collateral damage} in the context of DDoS prevention.
In theory, the feedback-loop-like model allows us to monitor flows \emph{after} an action is taken to allow forgiveness of mistakenly punished flows.
This does rely upon the ability to take a flow-by-flow view of the state space, but if we can combine knowledge of current state with the last applied action, then perhaps a flow which falls off identically to a legitimate flow can be rescued.

%Which features might be best suited to this problem?
%?? Relevant features: aggregate network state (load at various points [this has been done, of course]), flow-specific measurements (upload/download ratio when bandwidth above threshold \cite{DBLP:conf/ndss/Rossow14}, packet inter-arrival times, etc.)
Other studies suggest that there are particularly useful features which make the task of online DDoS flow identification feasible.
Aggregate network load observed at various locations suggests the overall health of a network \cite{DBLP:journals/eaai/MalialisK15}, and the ratio of correspondence between pair flows can suggest asymmetry and in many cases illegitimacy \cite{DBLP:conf/ndss/Rossow14}.
Generic volume-based statistics (counts, counts per duration, average packet sizes) have seen effectiveness in such as $k$-nearest neighbours classifiers trained to detect DDoS attacks in progress \cite{DBLP:conf/dsn/LeeKSPY17}.
Most importantly, there is evidence showing behavioural changes in response to bandwidth expansion \cite{DBLP:conf/ndss/KangGS16}, suggesting similar artefacts might arise after throttling, packet drop, or other interference.
%?? If we assume amplification attacks, we know it won't be `random' source IPs (since it's mostly-legit servers who think that they're doing a good job by replying)
%?? If we assume amplification attacks, we know it won't be `random' source IPs (since it's mostly-legit servers who think that they're doing a good job by replying)

%\section{A Plan, of Sorts}
%
%\begin{enumerate}
%	\item The main case for contribution in what I have so far:
%	\begin{itemize}
%		\item Past work reliant on unrealistic network models: tcp-like behaviour (and its effects on collateral damage) not captured, disjoint ranges of traffic distribution (no benign heavy-hitters), ISP-like topology.
%		\item I offer more realistic network emulation environment, better treatment of protocol/traffic characteristics.
%	\end{itemize}
%	\item Forthcoming: rethinking state/action spaces to operate at a finer level of granularity. New network model (live tcp back-and-forth), allows us to test collateral damage assumptions in a more realistic manner (and show clear case for moving beyond work of malialis and kudenko)
%\end{enumerate}

\subsection{Threat Model}

%?? The fine folks at most security conferences will want to see one. I suppose it serves its purpose by delineating what an attacker can/cannot do.

%?? Oultine attacker's goals, particularly wrt SPIFFY.
%?? They want to impact fair-share bandwidth allocation to flows (minimise), they have no worry about being unmasked (there are many available reflectors, which don't need to be bought etc...) [cite the rossow papers about amplification hell].
An attacker's goal is to minimise the fair-share bandwidth allocation that a server can give to any host which connects to it, and they are expected to act rationally in its pursuit.
Threat actors are external and act intentionally, aren't expected to be classified as an \emph{advanced persistent threat}, and likely range from hacktivists to moderately funded adversaries.
We assume that attacks will be bandwidth-based DDoS attacks with the structure of an \emph{amplification attack}, and that traffic aggregates at the target (unlike in a transit-link attack).
The addresses of any machines taking part in an attack are not revealed to the target, save for the fairly stable set of unwitting reflector nodes.
The discovery of any reflector by some defence system does have a cost to the attacker---there is a particularly large (yet finite) supply of viable reflector nodes \cite{DBLP:conf/ndss/Rossow14}, but the constraints that each has a large upstream bandwidth and support for high-amplification-factor protocols narrow this pool.

We do not assume that an attacker has white-box access to the parameters underlying an agent's policy, or that they will attempt to intelligently modify flow/system state to indirectly control an agent \cite{DBLP:conf/eurosp/PapernotMJFCS16, DBLP:conf/eurosp/PapernotMSW18, DBLP:journals/corr/HuangPGDA17, DBLP:conf/sp/Carlini017}.
While they may be able to perform some degree of reverse engineering by observing the health of their own legitimate canary flows, ``stealing'' the policy through observation \cite{DBLP:conf/uss/TramerZJRR16}, investigating whether perturbations would persist in a medium as volatile as network traffic statistics falls outside of the scope of this work.
The same observation extends to the possibility of poisoning attacks \cite{DBLP:journals/jmlr/KloftL10, DBLP:conf/acsac/ShenTS16}; to the best of our knowledge, no studies have been undertaken surrounding the mis-training of RL agents.
These are APT-level capabilities, whose exploration presents a rich source of future work.

\section{Analysing Existing Work}\label{sec:environment-and-rl-algorithm}
%?? State the problem and what we want to achieve.
To best discover how the use of RL techniques in the field of network intrusion prevention may be improved, it's prudent that we start by looking for weaknesses in work which exists today.
The closest available approach within this field is that of \textcite{DBLP:journals/eaai/MalialisK15}, and their contribution in applying RL to the task of intrusion prevention is significant: their work helps to show the viability of live, adaptive, feedback-loop-like control of the network to detect and prevent DDoS attacks.
We begin with a reimplementation and more principled re-examination of their work to further develop the state-of-the-art.

%?? Explicitly say the ``REIMPLEMENTING M\&K THING'' over here, to make later paras make a reasonable amount of sense.

%?? Why are we comparing against/building on/reimplementing M\&K? What was their main contribution? Answer: they helped to show the viability of live, adaptive control of the network to detect/prevent DDoS attacks...

\subsection{Topology}\label{sec:topology}

The network itself is tree-structured, where one server $s$ connects through a dedicated switch to $k$ team leader switches, each connected to $l$ intermediate switches, which in turn each connect to $m$ egress switches.
Agents are co-located with these egress switches, who control the proportion of upstream packets from $n$ external hosts to discard according to load statistics observed along their path to the server.
Agents communicate with their co-hosted OpenFlow-enabled switches---running a modified version of \emph{Open vSwitch} (OVS) \cite{open-vswitch}---to install probabilistic packet-drop rules.
Each link has a delay of \SI{10}{\milli\second}.
All links have unbounded capacity, save for the server-switch connection which is capped at a fixed $U_s$ \si{\mega\bit\per\second} in both directions.
\Cref{fig:marl-topol} demonstrates this visually.

\begin{figure}
\centering
\resizebox{\linewidth}{!}{
				\begin{tikzpicture}[
texts/.style = {text=black},
labeltexts/.style = {text=uofgsandstone},
treeline/.style = {draw=uofgburgundy},
treenode/.style = {texts, circle, centered, fill=white, treeline},
load/.style = {fill=uofgcobalt},
loadhide/.style = {fill=uofgcobalt!40!white},
external/.style = {fill=uofgrust},
externalhide/.style = {fill=uofgrust!40!white},
hideline/.style = {draw=uofgsandstone!40!white},
hidenode/.style = {treenode, hideline},
grow'=right
]
\node[treenode, label={[texts]above:Server}] (root) {}
child [treeline] { node [treenode, load, label={[texts]above:Core}] (sswitch) {}
	child [treeline] { node [treenode, load, label={[texts]above:Leader}] (teaml) {} 
		child [treeline] { node [treenode, load, label={[texts]above:Intermediate}] (inter) {}
			child [treeline] { node [treenode, load, label={[texts]above:Agent/Egress}] (agent) {}
				child [treeline] { node [treenode, external] (extern) {}
					child [treeline] { node [treenode, external, label={[texts]above:Host}] (host) {} }
					child [hideline] { node [hidenode, externalhide] (endhost) {} }
				}
			}
			child [hideline] { node [hidenode, loadhide] (endagent) {} }
		}
		child [hideline] { node [hidenode, loadhide] (endinter) {} }
	}
	child [hideline] { node [hidenode, loadhide] (endteaml) {} }
	edge from parent
	node[below, labeltexts] {$U_s$}
};

%\draw[-] (teaml) -- (endteaml);
\node [labeltexts] (kdots) at ($(teaml)!0.5!(endteaml)$) {$\rvdots$};
\node [labeltexts, right = -0.1cm of kdots] {$k$};
\node [labeltexts] (ldots) at ($(inter)!0.5!(endinter)$) {$\rvdots$};
\node [labeltexts, right = -0.1cm of ldots] {$l$};
\node [labeltexts] (mdots) at ($(agent)!0.5!(endagent)$) {$\rvdots$};
\node [labeltexts, right = -0.1cm of mdots] {$m$};
\node [labeltexts] (ndots) at ($(host)!0.5!(endhost)$) {$\rvdots$};
\node [labeltexts, right = -0.1cm of ndots] {$n$};
\end{tikzpicture}
}
\caption{
	Network topology diagram, showing how the server and its core switch's $k$ teams are structured, with $l$ intermediate routers per team, connected to $m$ agents which each moderate $n$ hosts beyond a single external switch.
%	Empty nodes are considered to be internal.
	Red nodes are considered to be external, and the load of each blue node features in the state vector.
	\label{fig:marl-topol}
}
\end{figure}

Why use this topology, a fan-out tree, in particular?
While this is taken verbatim from the work we use as a basis, it is presented within the confines of a framework given by \textcite{DBLP:journals/ton/YauLLY05}.
This framework is, in turn, an extension of the class of topologies examined in the original pushback paper \cite{DBLP:journals/ccr/MahajanBFIPS02a}; this topology is far closer to the latter.
Crucially, it seems to not actually represent or correspond to any realistic deployment at all.
In the words of \citeauthor{DBLP:journals/ccr/MahajanBFIPS02a}:
\begin{displaycquote}{DBLP:journals/ccr/MahajanBFIPS02a}
	These simulations do not pretend to use realistic topologies or traffic mixes... the simple simulations in this scenario are instead intended... as a first step towards more rigorous evaluation.
\end{displaycquote}
%The topology chosen to evaluate our predecessor, indeed, matches theirs.
%?? So, why does this proliferate, exist? They effectively disavowed it in their own paper. Reading comprehension is hard, I guess.
%Remarkably, it seems to have propagated in a way which was not originally intended.
It is clear that such a network topology allows for functional testing, and indeed is illustrative of one way in which attack traffic might aggregate in the network, but it is hard to argue its relevance to specific classes of victim or to reason about the interactions it might have with dependent applications.
We aim to address this, to some extent.

Reimplementation of this network model was performed using mininet \cite{mininet}, a Python-based network emulator.
Traffic is played back from hosts via Tcpreplay at a bandwidth assigned uniformly from a `good' or `bad' distribution, each using the same pcap file with source and destination IP addresses rewritten.
While unrealistic with respect to packet content, it was assumed that this would provide representative load and packet inter-arrival characteristics, since the agents as defined rely only upon load measurements.

\subsection{Agent Definitions}

Agents receive state input from monitors in the environment, a vector of the four load values (in \si{\mega\bit\per\second}) observed along their path to $s$: this is tile-coded with identical parameter choices to those of \textcite{DBLP:phd/ethos/Malialis14}.

At every time step, each agent installs an action via OpenFlow, instructing its host switch to drop each packet with probability $p$.
Specifically, the agent chooses $p \in \left\{ 0.0, 0.1, \ldots, 0.9 \right\}$, giving a discrete, static action set which cannot completely filter traffic.
In essence, this means that each agent is in control of pushback \cite{DBLP:journals/ccr/MahajanBFIPS02a}.
It's worth noting that there are various ways that this could be done, and that the application of \emph{programmable data planes} to this end are suggested as future work.

We employ the reward functions introduced by \textcite{DBLP:journals/eaai/MalialisK15}; in particular, we make use of their global reward as a performance metric and their \emph{coordinated team learning} (CTL) reward for training.
These assume the presence of a classifier, $g(\cdot)$, which estimates the volume of `legitimate traffic' received at any point in the network.
At time $t$ an agent $v$ (with team leader $l_v$) receives either $R_{t}^{\mathit{Global}}$ or $R_{t}^{\mathit{CTL}}$:
\begin{subequations}
\newcommand{\cond}[2]{\operatorname{c}_{#1,t}#2}
\newcommand{\load}[1]{\operatorname{load}_{t}(#1)}
\begin{gather}
\cond{f} = [\load{s} > U_s],\\
\cond{c}{(v)} = \cond{f} \cdot{} \, [\load{l_v} > U_s/k],\\
R_{t}^{\mathit{Global}} = (1 - \cond{f}) \frac{g(\load{s})}{g(\load{\mathit{total}})} - \cond{f},\label{eqn:reward-rt}\\
R_{t}^{\mathit{CTL}}(v) = (1 - \cond{c}{(v)}) \frac{g(\load{l_v})}{g(\load{\mathit{team}})} \allowbreak - \cond{c}{(v)},\label{eqn:reward-rctl}
\end{gather}
where $\load{\mathit{total}}$ and $\load{\mathit{team}}$ refer to the load across all egress points in the network and in $v$'s team, respectively.\label{eqn:reward}
\end{subequations}

The tile-coded state and chosen reward function are then used as input for the \emph{semi-gradient Sarsa} algorithm described in \cref{eqn:sg-sarsa}, making use of $\epsilon$-greedy action selection.
Each agent has its own internal parameter vector $\wvec{}$, and agents do not share their experience or weight vector updates with one another.

%?? M\&K Seem not to have a grasp on the importance of up/down traffic splits. Hmm......... (i.e., both could be relevant?) Man, who knows.

%?? wrt reward function relying on heuristic elements or perfect knowledge:
While it can be observed that reliance upon heuristic elements or perfect knowledge as in the above reward functions hampers their real-world applicability, a sufficiently well-trained agent needs only to follow the policy it has learned in a greedy manner, allowing pre-training by a simulated environment to transfer to reality.
%?? BUT, if we don't have reward functions which are available at runtime, then we've lost out on what is arguably one of the main benefits that RL can give us!
%?? Need to tie in the concept of ``meaningful work''?
The lack of a truly online reward function is sorely felt, however.
This causes the loss of some of RL's main benefits, such as online learning with regard to an evolving problem context.

\section{Performance in an Emulated Environment}\label{sec:performance-in-an-emulated-environment}
The original introduction of this approach to direct-control reinforcement learning as introduced by \textcite{DBLP:journals/eaai/MalialisK15} fails to consider key cases: the absence of a suitable heuristic classifier $g(\cdot)$, disjoint ranges of traffic distribution (i.e., the presence of benign heavy-hitters), the accurate simulation of TCP-like behaviour (and its effects on collateral damage), and assumes a rigid and unrepresentative topology.
The latter two are most deserving of a closer investigation, as they have stronger implications for wide-scale deployment.

All experiments described were executed on Ubuntu 16.04.5 LTS (GNU/Linux 4.4.3-040403-generic x86\_64), using a 4-core Intel Core i7-6700K (clocked at \SI{4.2}{\giga\hertz}) and \SI{32}{\gibi\byte} of RAM.
All code underpinning these findings is available on a public repository\footnote{Link removed for double-blind reviewing.}.
%All code underpinning these findings is available on a public repository\footnote{\url{https://github.com/FelixMcFelix/rln-dc-ddos-paper}}.

%?? TODO: Make repo public...

\subsection{Service Guarantees and Host Density}
The default topology chooses $n=2$ hosts per egress point, which isn't a parameter setting that can be described as realistic for many web service deployments.
In practice, well-established network topologies such as the fat-tree topology fan out \emph{internally} (to provide efficient replication and scaling of resources) while having comparatively few egress points.
We feel is most worth examining aspects of such a case, where the host/agent ratio is heavily skewed in favour of hosts and actions (as defined above) become less granular.

To test this, we configured the network topology using $k=2$ teams, $l=3$ intermediate nodes per team, $m=2$ agents per intermediate node, and $n \in \{2, 4, 8, 16\}$ hosts per learner.
This is a slight simplification of the \textquote{online} experiment as presented by \textcite{DBLP:journals/eaai/MalialisK15}, choosing fewer teams but remaining as a single-server with a fan-out network.
The algorithm parameters were set at $\gamma=0$ (leading to opportunistic behaviour), $\alpha=0.05$, having linearly annealed $\epsilon=0.2 \rightarrow 0$ by $t=3000$.
Benign and malicious hosts uploaded between \SIrange{0}{1}{\mega\bit\per\second} and \SIrange{2.5}{6}{\mega\bit\per\second} respectively, and hosts were redrawn at each episode's start with $\operatorname{P}(\mathit{malicious})=0.4$.
$U_s$ was fixed at $klmn+2$ \si{\mega\bit\per\second}.
The performance of each choice of $n$ was averaged over \num{10} episodes of length \num{10000} timesteps (setting each agent's $\wvec{}=\bm{0}$ between episodes).
Host allocations were generated pseudorandomly to ensure fairness between choices of $n$.

\begin{figure}
	\includegraphics[width=\linewidth]{../plots/online-varyN-binary}
	\caption{
		MARL pushback control system performance plotted over time for various settings of $n$ hosts per agent.
		This plot shows that, from the perspective of benign hosts, service guarantees degrade as inference and actions become less granular.
		This generalises for all behavioural discriminators: even a perfect agent \emph{must} punish benign flows if they are grouped with malicious actors.
		%		?? Effect on max perf AND learning rate??
		Crucially, both the maximum achievable performance and learning rate are negatively impacted when granularity is coarse.
		\label{fig:marl-granularity}
	}
\end{figure}

\Cref{fig:marl-granularity} shows that as more hosts are allocated to each learner, the fraction of traffic believed to be good (as observed at the server) decreases.
This manifests in two ways: the best-achievable performance drops, and so too does the learning rate.
The first consequence arises analytically.
Given the probability that a host is legitimate, $P_G \in [0,1]$, it follows that a host will be malicious with probability $P_B = 1 - P_G$.
Defining \emph{imperfect service} to mean any case where all $n$ hosts connecting over a switch do not share the same classification (i.e., a mixture), then the probability that a switch is delivering imperfect service is $P_{M,n} = 1 - (P_G^n + P_B^n)$.
\begin{thm}
	As $n$ increases, it is more likely that a throttling switch will exhibit imperfect service: $\forall n \in \mathbb{Z}^{+}, P_{M,n} \le P_{M,n+1}$.
\end{thm}
\begin{proof}
	\emph{Base case:} $P_{M,1}=0, P_{M,2} = 1 - P_G^2 - P_B^2 > 0$.
	\emph{Inductive step:} Assume that the theorem holds for $n$. Observe that $P_G^n \ge P_G^{n+1}$ (resp.\ $P_B$). It then follows that:
	\begin{align*}
	P_G^n + P_B^n &\ge P_G^{n+1} + P_B^{n+1}\\
	1 - (P_G^n + P_B^n) &\le 1 - (P_G^{n+1} + P_B^{n+1})\\
	P_{M,n} &\le P_{M,n+1}
	\end{align*}
\end{proof}
\begin{corr}
	Restricting $P_G \in (0,1)$ so that both $P_G$ and $P_B$ are non-zero ensures strict inequality: $P_{M,n} < P_{M,n+1}$.
\end{corr}

%?? Expand according to the summary you posted in the notes document.
%That the use of UDP-like traffic replicates their experiments, compared against how real tcp behaves in mininet and according to the Mathis equation, shows that they really did not consider the behaviour of benign TCP in this environ (i.e., their results should be considerably worse in practice).

\subsection{TCP Multiplexing and Falloff Behaviour}\label{sec:tcp-multiplexing-and-fallback-behaviour}

%While Tcpreplay was useful in recreating the original work and examining the above condition, flow behaviour under packet loss is closer to that of UDP when using it to replay traffic.
%To investigate the effects of collateral damage upon benign TCP flows, an iperf3 test was conducted using mininet.
%Several hosts communicated concurrently to a single server over one switch, with maximum send rates in $\{1,...,5\}$ \si{\mega\bit\per\second} and a constant \SI{40}{\milli\second} latency.
%The rate of packet drop was increased linearly from \SIrange{0}{40}{\percent} and the observed throughput for each host was measured, to show a trend and determine for the mininet environment whether throughput would decrease as expected, at a super-linear rate.

%\begin{figure}
%	\includegraphics[width=\linewidth]{../plots/mplex}
%	\caption{
%		Average TCP upload rate of hosts targeting different bandwidths in \emph{mininet}, computed over 10 runs, for choices of packet drop rate up until throughput is reduced to almost \SI{0}{\mega\bit\per\second}.
%		As the rate of packet drop increases from \SIrange{0}{40}{\percent}, all flows converge on an extremely low throughput, regardless of the target bandwidth---a super-linear decrease for legitimate TCP traffic.
%		Prior treatments have ignored such phenomena, which have strong implications on agent and network design.
%		Malicious flows are not expected to obey the Mathis equation in this way, worsening collateral damage experienced by benign TCP flows.
%		\label{fig:mplex-tcp}
%	}
%\end{figure}

\begin{figure}
	\includegraphics[width=\linewidth]{../plots/online-varyN-nginx}
	\caption{
		Comparison of preserved traffic across different traffic models, choosing $n=2$ hosts per egress node.
		No actions are taken in the ``baseline'' entries.
		The agents examined here perform above the baseline in all cases, but perform significantly worse for live TCP traffic (nginx).
		\label{fig:nginx-coffin-nail}
	}
\end{figure}

%\Cref{fig:mplex-tcp} shows that this trend is present.
Given the UDP-like behaviour of Tcpreplay, being able to replicate the online performance of \citeauthor{DBLP:journals/eaai/MalialisK15} in the $n=2$ case suggests that the numeric simulation their work is based upon does not capture the key interactions between TCP flows and packet drop as a control action.
The TCP traffic as it appears here is generated according to \Cref{sec:a-new-normal}.
From the Mathis equation \cite{DBLP:journals/ccr/MathisSMO97}, through which the performance characteristics of TCP under packet drop are well-known and well-understood, their results should be considerably worse in practice:
\begin{equation}
\operatorname{BW} = \frac{\operatorname{MSS}}{\operatorname{RTT}} \frac{C}{\sqrt{p}},
\end{equation}
for the \emph{maximum segment size} (MSS), \emph{round trip time} (RTT), a choice of constant $C \approx{} \sqrt{3/2}$ dependent on system assumptions, and the probability $p \in (0, 1]$ that a packet is lost.
Accordingly, benign TCP flows are severely harmed by \emph{any} packet drop, so collateral damage to these hosts is more significant; good-faith TCP congestion avoidance causes these flows to attempt to back off very quickly.
\Cref{fig:nginx-coffin-nail} shows that this model performs poorly when attempting to safeguard TCP traffic, though remains better than leaving the network unprotected.
%?? Talk about the same major points you made in the presentation here (i.e. TCP interactions) w/ pushback. Or in the evaluation...
UDP, however, behaves as expected---malicious hosts are expected to exhibit the same behaviour, given that congestion is their primary goal.

These results are of particular importance due to TCP's prevalence within the Internet.
The protocol is known to carry \SIrange{79}{94}{\percent} of packets, corresponding to \SIrange{89}{98}{\percent} of data transferred by volume \cite{DBLP:conf/saint/ZhangDJC09}.
As far as future network protocols are concerned, QUIC \cite{DBLP:conf/sigcomm/LangleyRWVKZYKS17}, a congestion-aware stream transmission protocol, will behave much like TCP, showing the importance of further development to properly handle traffic with such characteristics.

\subsection{Computational Cost}
%?? Consider talking about the execution times of the old MARL approach here? They're real nice (as expected), so we have lots of room to play around with while (hopefully) remaining under the 1ms target time given by \textcite{DBLP:conf/sigcomm/ChenL0L18}.

Overall, each episode takes around \SI{10}{\minute} to run, while each set of \num{10} requires around \SI{2}{\hour} due to additional set-up/tear-down costs associated with mininet.
Measurements taken during each of these experiments indicated that the cost of computing any action is typically within \SIrange{80}{100}{\micro\second}.
This is reassuring when measured alongside the insights from other work.
\Textcite{DBLP:conf/sigcomm/ChenL0L18} observe that, ideally, actions must be computed and taken within \SI{1}{\milli\second} to have a meaningful affect on short flows.
%Most flows are short, and flow-size follows a heavy-tailed distribution.
That our starting point falls significantly below this threshold allows us to safely consider more costly actions or larger state spaces, which would typically increase the computational cost.

%\subsection{Synthesis}
%%It'd be really good if I could figure out a rough proof for the service property...
%%Okay... now say what this means for future viability! What changes can I make? Flow-specific metrics, of course! Increase granularity as much as I can!
%Both of these experiments suggest that the most viable way to enhance and deploy this MARL approach is to consider flows individually to maximise performance for legitimate hosts and to detect malicious hosts in a more principled manner.
%While this will add significant computational cost, this may be worked around with intelligent sampling and monitoring while taking multiple actions per-timestep \cite{DBLP:conf/hotnets/MaoAMK16}.
%
%?? Observations: training time lengthier by nature of emulated environment.
%?? Risk of going for a simulation (i.e. numeric)? Always going to be interesting behaviour that is missed out on (in theory), at the cost of training time/limits of simulation speed

\section{A New Normal}\label{sec:a-new-normal}

%In establishing...
%
%?? How will I structure this?
%?? Motivation -> Model -> Results?
%?? OR Use the results of the last section to springboard into here?

From what we have seen, it is difficult (or impossible) for trace-based or numerical simulations to correctly capture certain dynamics without an extraordinary amount of care or consideration.
%As it turns out, 
Our goal is to briefly describe an environment which tests \emph{specific} behaviours to examine the \emph{specific} problems which have arisen during our testing of past approaches.
In particular, we are interested in capturing interactive, correlated back-and-forth exchanges associated with live HTTP traffic; mainly because of the particular interactions between the application-level dynamics, congestion awareness at the transport level and the nature of control signal used.
We make no claims that it is perfect or representative for all traffic, only that it captures some of the behaviour which we expect will plague most legitimate TCP flows; if need be, we expect the frequency or distribution of requests could be conditioned to match observations of real-world access patterns.

%?? ANGLE: set up an environment to test \emph{specific} behaviours to examine \emph{specific} problems in past work. I make no claims that it is perfect or representative for all traffic, just for this (likely common) behaviour which I expect to plague almost all legit TCP flows.

%?? Existing sims used for testing such applications reliant on traces, or not sophisticated enough to capture interactive, back-and-forth (correlated) behaviours---possibly discarded as second-hand effects by past work when these are so crucial given user traffic patterns (and the nature of the control signal we choose to enact).

%?? Remember, the motivation is clear. We don't care so much that it is "representative" wrt a specific deployment location or network type. The whole purpose of this is that we aim to test specific behaviour which traces cannot replicate (i.e., correlated back-and-forth, dynamics introduced to congestion-aware protocols, ...)
%?? If we need to, we can condition the distribution of requests according to statistics mined from an existing trace if reviewer number 2 needs that extra push to be convinced.

\subsection{Network Design}
We make use of a fully software-defined network, built using Openflow-aware switches in mininet alongside a controller application based on \emph{Ryu} \cite{ryu}.
All internal routers are primed with knowledge of the shortest path to each internal host, while new inbound flows register the ``way back'' for each hop used, to ensure consistent traffic conditions for each flow.
If such information is lost, perhaps expiring due to inactivity, it suffices to forward an outbound packet on a random (outbound) port, as we assume that any external IP is reachable through any of the test network's egress ports (i.e., that it is not connected to any stub autonomous systems).
The controller is also responsible for computing how switches respond to ARP requests: the necessity of this arises due to the reliance upon Linux's networking stack for live applications, and wouldn't need to be considered for purely trace-based evaluation.
We make further use of the topology presented earlier (\cref{sec:topology}), noting that our architecture allows us to trivially extend and modify this if required.

\subsection{Traffic Model}
%?? Legitimate traffic: TCP traffic (HTTP clients downloading web pages, dependent resources and files) with a mixture of lifetimes for each request.
To model legitimate traffic the server node runs an nginx v1.10.3 HTTP daemon, serving statically generated web pages alongside various large files and binaries.
Benign hosts run a simple libcurl-based application written in Rust, repeatedly requesting resources from the server.
Each host's download rate is limited to match whatever maximum bandwidth was assigned to it.
This provides two modes of operation:
\begin{itemize}
	\item \emph{long flow} testing has each host constantly request a large binary file, such as a tarball of GCC8;
	\item \emph{request-chain} testing has each host request several random files known to exist within the website, followed by any dependent resources for each (stylesheets, images, etc.). On completion, a host changes its IP to generate separate statistics per-flow, while minimising downtime. This presents a more balanced distribution of flow duration and size.
\end{itemize}

%?? Malicious traffic: UDP flood traffic (hping3, MTU-size packets, ). Why not min-size packets? Because the traffic generator gets in a horrible rut if I do so...
Malicious traffic is generated by use of the \emph{hping3} program, generating UDP-flood traffic targeting random ports.
Each malicious packet is MTU-sized, loosely mapping to the large frame sizes expected in an amplification attack and to ensure that simulation remains tractable.
When using TCP-like traffic, we must increase each attacker's upload range to \SIrange{4}{7}{\mega\bit\per\second} to meaningfully impact benign flows.

\section{Acting On Individual Sources}
Our main hypothesis is that the best method for advancing past the current shortcomings of RL-based DDoS mitigation is to modify the design of agents such that filtering decisions are computed per source.
However, any of these alterations must account for computational constraints imposed by the deployment environment---the amount of flows passing over an agent is unbounded.
We describe and justify our enhancements to the RL model of \textcite{DBLP:journals/eaai/MalialisK15}, present a new model which draws on domain knowledge leveraged by SPIFFY \cite{DBLP:conf/ndss/KangGS16}, and offer an empirical investigation into the effectiveness of flow-level statistics on live decision-making.

\subsection{Marl++}\label{sec:marl-plus}
%\fakepara{Multiple actions}
%?? Talk about the modifications to standard: multiple actions per-timestep, updating many traces from one reward...
Modifying the frequency with which actions and value functions are updated has important consequences for not only overall system performance, but also in \emph{how} an agent learns.
We choose to adapt these algorithms to prioritise rapid response to changes in network state and to visit as many state-to-state transitions as possible to facilitate effective learning.
To this end, we allow agents to make many decisions per timestep.
We maintain the last state-action pair associated with each source IP, and calculate any actions for the flows which still exist.
Finally, we update $\wvec{}$ according to \cref{eqn:sg-sarsa} with each available trace and the sole reward measurement an agent has at that time.
As exploration still occurs for each action, this approach reduces $\epsilon$ multiple notches every timestep.
In turn, we increase the annealing window for $\epsilon$ to \num{8000} action choices so as to preserve exploration over time, by accounting for the greater volume of decisions being made.

%\fakepara{Timed Random Sequential actions}
Taking actions in this manner means that any agents are assigned a larger, and potentially unbounded, set of tasks to perform every time they receive load and flow statistics from the network and their parent switch.
This introduces numerous potential issues: inability to response to unexpected changes in flow state, delayed service of new flows, and risks that flow states become outdated; at their worst, these present additional attack surface to an adversary.
To adapt to these problems, we make use of \emph{timed random sequential} updates.
Each agent begins with an empty work list.
For the set of flows active in any timestep, we shuffle the list and perform as many action calculations and updates as possible, within a set time limit.
Uncompleted work is passed on to the next timestep, until it is emptied, at which point is is repopulated using the set of available measurements.
To ensure that flow control actions are made with recent information, we combine state vectors for unvisited flows in the current work set, and replace the stored vector for all others.
State vector combination is done by summing deltas and packet counts, updating means via weighted sums, and replacing all other fields with the newest measurements.
Following \textcite{DBLP:conf/sigcomm/ChenL0L18}'s observations concerning short flows, we maintain a deadline of \SI{1}{\milli\second}---in tests, an agent is typically able to process around 8 flows in this time.
We expect this should be tuned based on the frequency at which statistics arrive.
%?? Prune old? We can get through 8 per iteration, might use this as a basis for knowing when to say ``enough is enough''.

\newcommand{\arrload}[2]{\operatorname{load}^{#2}_{t}(#1)}
\newcommand{\uload}[1]{\arrload{#1}{\uparrow}}
\newcommand{\dload}[1]{\arrload{#1}{\downarrow}}
\newcommand{\bload}[1]{\arrload{#1}{\updownarrow}}
\newcommand{\cond}[2]{\operatorname{c}_{#1,t}#2}
%\fakepara{Reward function directionality}
The reward functions, as defined, do not take traffic direction into account.
We redefine these to identify overload states using both upstream and downstream loads, while allowing customisation of which direction is chosen for protection.
Denoting the upstream, downstream and combined loads $\uload{s}, \dload{s}, \bload{s}$, we modify \cref{eqn:reward}:
\begin{subequations}
\begin{gather}
\cond{f} = [\max(\uload{s}, \dload{s}) > U_s],\\
\cond{c}{(v)} = \cond{f}{} \cdot{} \, [\max(\uload{l_v}, \dload{l_v}) > U_s/k],
\end{gather}
\label{eqn:reward-but-better}
\end{subequations}
replacing $\arrload{t}{}$ in \crefrange{eqn:reward-rt}{eqn:reward-rctl} with whichever directional load is prioritised.
We choose $\uload{\cdot}$ for our UDP-based models and $\dload{\cdot}$ for HTTP, though we expect that $\bload{\cdot}$ would be the most suitable for general deployment or heterogeneous traffic patterns.


%Explicitly mention \emph{why} we don't make use of neural networks, and instead favour more classical function approximation. Latency concerns, energy usage, graphics/TPU/whatever hardware availability or lack thereof across switches/routers/etc.
To make decisions cheaply and at low latency, we maintain the usage of semi-gradient Sarsa with tile coding, rather than making use of neural networks or more complicated function approximators.
Although this likely brings lower theoretical performance, there are more reasons than latency alone to favour classical methods; these include lower energy usage, reduced model complexity (and training time), the availability of necessary hardware, and simpler decision boundaries.

\subsection{SPF: SPIFFY Lite}

An important question to ask while exploring the design space of RL agents is whether (and how) domain knowledge can improve the performance or learning rate of a defence system.
This often comes as a trade-off: expressive power and unfettered control, weighed against the smaller action spaces afforded by existing insight or structure.
We propose a second model, relying on the measurements of \textcite{DBLP:conf/ndss/KangGS16} which suggest that bot attack flows cannot scale up to match an increase in available bandwidth.
While it is not stated whether this behaviour applies to amplification attacks, we expect this to remain the case; the upload rate of any amplifier will be fixed at its upper bound given that the upstream links of any connected bots are saturated.
Moreover, bots receive no feedback on the effective throughput of their attack flows.

How might we apply their observations within the RL paradigm, while tailoring them towards non-ISP deployment?
Our approach is to constrain how an agent treats each flow using a simple finite state automaton: we restrict $p \in \left\{ 0.00, 0.05, 0.25, 0.50, 1.0 \right\}$, reducing the action set to \emph{maintain}, \emph{increase}, or \emph{decrease} $p$ in single steps.
To enable temporary bandwidth expansion in resource constrained deployments, every flow is initially placed under light packet drop ($p=0.05$, or state 1), corresponding to roughly a \SI{50}{\percent} rate reduction for TCP flows in our test topology.
This uneven spread of choices for $p$ allows light and heavy rate reduction to be applied to both congestion-aware and non-congestion-aware traffic as required.
We combine this with the enhancements described in \cref{sec:marl-plus}.

This model is significantly different when considering how to tune the RL algorithm's parameters: each agent now requires the capability to plan ahead, since they may no longer choose $p$ directly from any state.
This entails choosing some discount factor $\gamma \ne 0$, allowing the value of future states to influence state-action value updates.
We found $\gamma = 0.8$ to be the most effective choice during exploratory testing with UDP traffic.

\subsection{Rethinking the State Space}\label{sec:rethinking-the-state-space}

\begin{figure*}
	\centering
	\begin{subfigure}{0.32\linewidth}
		\includegraphics[width=\linewidth]{../plots/ftprep-good}
	\end{subfigure}
	\begin{subfigure}{0.32\linewidth}
		\includegraphics[width=\linewidth]{../plots/ftprep-good-2}
	\end{subfigure}
	\begin{subfigure}{0.32\linewidth}
		\includegraphics[width=\linewidth]{../plots/ftprep-good-3}
	\end{subfigure}
	\caption{
%		Features: how good are they really? These are for UDP. As you can see, Most things are pretty average (except for \emph{Mean IAT}, which dominates).
		Learned performance of Marl++ agents when benign traffic is UDP-like, using only a single feature as a basis for decisions.
		Mean IAT, inbound packet sizes, and global state offer the best predictive performance, while most features offer marginal advantage over the unprotected baseline.
		Here, the link's bandwidth limit is removed, and so agents automatically learn to keep bandwidth below $U_s$ by negative reinforcement---this offers a slight per-feature performance increase compared to when this explicit cap is present (not pictured).
		\label{fig:udp-feature-plots}
	}
\end{figure*}

%\begin{figure*}
%	\centering
%	\begin{subfigure}{0.32\linewidth}
%		\includegraphics[width=\linewidth]{../plots/ftprep-cap-good}
%	\end{subfigure}
%	\begin{subfigure}{0.32\linewidth}
%		\includegraphics[width=\linewidth]{../plots/ftprep-cap-good-2}
%	\end{subfigure}
%	\begin{subfigure}{0.32\linewidth}
%		\includegraphics[width=\linewidth]{../plots/ftprep-cap-good-3}
%	\end{subfigure}
%	\caption{
%		Features: how good are they really? These are for UDP, but capped. As you can see, Most things are pretty average (except for \emph{Mean IAT}, which dominates).
%		\label{fig:udp-cap-feature-plots}
%	}
%\end{figure*}

\begin{figure*}
	\begin{subfigure}{0.32\linewidth}
		\includegraphics[width=\linewidth]{../plots/ftprep-tcp-good}
	\end{subfigure}
	\begin{subfigure}{0.32\linewidth}
		\includegraphics[width=\linewidth]{../plots/ftprep-tcp-good-2}
	\end{subfigure}
	\begin{subfigure}{0.32\linewidth}
		\includegraphics[width=\linewidth]{../plots/ftprep-tcp-good-3}
	\end{subfigure}
	
	\caption{
%		Features: how good are they really (part 2)? These are for TCP. These are less conclusive...
		Learned performance of Marl++ agents when benign traffic is TCP-like, using only a single feature as a basis for decisions (uncapped as in \cref{fig:udp-feature-plots}).
		Global state and Mean IAT still offer the greatest improvement above baseline, but it is observed that different optimal features are expressed between different traffic models.
		Packet-level statistics are considerably less effective for this class of traffic, with $\Delta$ Out Rate and Correspondence Ratio being proportionally better than they are alongside UDP.
		\label{fig:tcp-feature-plots}
	}
\end{figure*}

\begin{figure*}
	\begin{subfigure}{0.32\linewidth}
		\includegraphics[width=\linewidth]{../plots/ftprep-tcp-cap-good}
	\end{subfigure}
	\begin{subfigure}{0.32\linewidth}
		\includegraphics[width=\linewidth]{../plots/ftprep-tcp-cap-good-2}
	\end{subfigure}
	\begin{subfigure}{0.32\linewidth}
		\includegraphics[width=\linewidth]{../plots/ftprep-tcp-cap-good-3}
	\end{subfigure}
	
	\caption{
%		Features: how good are they really (part 3)? These are for TCP, but capped. These are less conclusive...
		Learned performance of Marl++ agents when benign traffic is TCP-like, using only a single feature as a basis for decisions (with bandwidth capped physically).
		Although negative rewards (and thus negative reinforcement) cannot occur, all features achieve considerably better performance, a reversal of our observation for UDP traffic.
%		Global state and Mean IAT still offer the greatest improvement above baseline, but packet-level statistics are considerably less effective for this class of traffic.
		\label{fig:tcp-cap-feature-plots}
	}
\end{figure*}

The main element required to move to a per-source model is a feature set which offers high predictive power, such that behavioural differences are readily apparent to an agent.
Elaborating further on the statistics discussed in \cref{sec:motivation} which others have shown to be effective, we believe the following features to be useful (and humanly justifiable), and offer an investigation into their use alongside different traffic types:
%?? We use these features, and why...

\fakepara{Global state ($\mathbf{4\cdot{}}$Load)}
These are the existing set of load measurements introduced in \cref{sec:environment-and-rl-algorithm}.
These indicate the overall health of the network, and crucially are all measurements which an agent directly controls.

\fakepara{Source IP address}
While ordinarily trivial to spoof, in real-world scenarios the addresses of a set of reflector nodes might exhibit similarity, most likely through geography.

\fakepara{Last action taken}
This encodes an agent's current belief in the maliciousness of a flow.
This feature also potentially allows forgiveness, serving as a reference point for determining whether a source mistakenly marked as malicious exhibits different falloff behaviour after punishment.

\fakepara{Flow duration and size}
Features which describe the length of time a connection has been active, and the amount of data transferred within that time.
An extraordinarily long flow, having sent a lot of data, could be more likely to be an amplifier.

\fakepara{Correspondence ratio}
The ratio between upstream and downstream traffic for a source IP.
We define this to be $C_X = \min(\uload{\cdot}, \dload{\cdot})/\max(\uload{\cdot}, \dload{\cdot})$, where a value close to 0 indicates strong asymmetry.

\fakepara{$\mathbf{\Delta}$ Send/receive rate}
The change in traffic rates caused by the last action.
Behavioural changes induced by bandwidth expansion/reduction are expected to be most visible in these fields.

\fakepara{Mean inter-arrival time (IAT)}
A measure of how often packets arrive at the agent's parent switch; low IATs indicate a high number of packets per second, and can be a possible marker of malicious behaviour.
We only make use of the mean IAT of \emph{inbound} traffic.

\fakepara{(Per-window) packet count}
The amount of packets sent to/from a source over a flow's lifetime (or the current window of measurement), similar in use to flow size and mean IAT.

\fakepara{Mean packet size per-window}
Legitimate flows, both TCP- and UDP-based, often transmit packets with a distribution of sizes.
Attack traffic is not likely to be so diverse: we might expect solely max-size packets in the case of amplification attacks, or minimum-size packets in other classes of flooding attack.

The exclusion of features such as source/destination ports or protocol numbers is a deliberate choice.
If \emph{QUIC} (or a similar protocol) were to become ubiquitous, then these fields would have little to no correlation with the class of traffic a flow might contain.
Our aim was to design around this constraint as a form of future-proofing.

\begin{table}
	\centering
	\caption{Tile coding windows for each feature.\label{tab:codings}}
	
	\begin{tabular}{@{}ll@{}}
		\toprule
		New Feature (unit) & Range \\
		\midrule
		Load (\si{\mega\bit\per\second}) & $[0, U_s]$ \\
		IP & $[0, 2^{32}-1]$ \\
		Last Action (\si{\percent}) & $[0, 1]$ \\
		Duration (\si{\milli\second}) & $[0, \num{2000}]$ \\
		Size (\si{\mebi\byte}) & $[0,10]$ \\
		Correspondence Ratio & $[0,1]$ \\
		Mean IAT (\si{\milli\second}) & $[0, \num{10000}]$ \\
		$\Delta$In/Out Rate (\si{\mega\bit\per\second}) & $[-50, 50]$ \\
		Packets In/Out & $[0, 7000]$ \\
		Packets In/Out Window & $[0, 2000]$ \\
		Mean In/Out Packet Size (\si{\byte}) & $[0, 1560]$ \\
		\bottomrule
	\end{tabular}
\end{table}

All of the above features, save for global state, are 1-dimensional.
Each is tile-coded with 8 tilings and 6 tiles per dimension, using the windows described in \cref{tab:codings}.
We chose to take a separate tile-coding over each feature, ignoring combinatorial effects at present to minimise the amount of extra work an agent must perform.
Indeed, combining too many features into one tile-coding can lead to poor performance due to an inability to generalise action values across similar states.
On the other hand, treating features individually allows us to reach a quick understanding of the importance of each while seeing adequate performance.

\Cref{fig:udp-feature-plots} shows the effectiveness of each feature for UDP (resp.\ \cref{fig:tcp-feature-plots} for TCP), using a topology with $n=2$ hosts per egress point averaged over 10 episodes.
%?? Core findings---different protocols need different features, so everything we proposed above has a use!
The plots demonstrate that different protocols and traffic classes are best defended by different features---as such, every feature presented has value in a complete model.
In general, some of the most effective features are the global state, mean IAT, mean inbound packet size and $\Delta$ rates.
%?? How do they do when combined after individual training? Pretty well, especially for TCP.
Additional testing shows that the learned per-feature policies may be easily combined (by summing action values), and that this technique is particularly effective for TCP; these results are omitted to preserve space.

%?? NEED TO SAY SOMEWHERE: Baselines use TC for establishing interleaving/buffers filling etc., when MARL+ or SPF are in play then disabling TC (thus allowing illegal states, with \num{-1} reward marking them as such---``negative reinforcement'') may accelerate learning. We have a choice---but the results show that for TCP we WANT TC enabled... For UDP, we actually get a slight degradation in performance. This suggests a kind of ``banding'', perhaps? Important to mention that it easily learns how to maintain a logical bandwidth limit.
During training of the above feature models, we elected to remove the physical bandwidth limit to allow agents to visit illegal states and receive \emph{negative reinforcement} for doing so.
It is important to note that this did not affect the agents' ability to keep the total load below $U_s$, as this behaviour can be easily learned in all cases.
%?? talk about cap difference: \cref{fig:tcp-cap-feature-plots}---we want cap enabled for TCP.
We found this led to a marginal improvement in efficacy when UDP was the primary source of benign traffic, but observed that this hindered learning when TCP was the primary protocol under protection (\cref{fig:tcp-cap-feature-plots}), likely due to the loss of key system dynamics.
It is anticipated that this difference might prove problematic for heterogeneous traffic mixtures.
One way of solving this may be to investigate the effect of replacing $U_s$ with e.g., $0.99 U_s$ in \cref{eqn:reward-but-better}, to introduce a small band of negative reinforcement without actually removing the load restriction in the environment.
Our intent is to potentially accelerate learning while preserving the effects of system dynamics---we term these \emph{banded} rewards.

%\subsection{What's done with these?}

%?? Tile coded together, action taken per-flow every timestep.

%?? NOTE: think about how I might need to change/alter the tile-coding when we add extra discriminative features... (i.e., are there better ways to handle covariance between properties of system/flow state, properly capture generalisation?)

%?? What other consequences are there? We're training each agent on all the flows it's acting on which means slight theoretical changes (we have multiple experience traces, and each uses its last state + new state + new target value (reward) as an update to the \emph{current model}).

\section{The Results of Doing So}
\label{sec:the-results-of-doing-so}

We now examine the performance of our two new models (\emph{Marl++}, \emph{SPF}) as compared against existing RL work (\emph{Marl}) under different traffic behaviour and varying $n$ as before.
%?? Probably best just to look at top level stuff, and THEN a simplified comparison for each intended improvement (like banded rewards).
Additionally, we examine the performance effects of environmental characteristics and potential improvements: negative reinforcement, the case where one agent makes all decisions, and pre-training on individual features.

We present the average rewards for all combinations of these factors in \cref{tab:av-vals}---these provide a rough idea of expected performance, giving the highest-performing model in bold.
Average rewards take into account any portions of time that an agent allows illegal system states.
Several plots augment this, illustrating peak performance or the amount of time which an agent requires to learn.

Unless stated otherwise, these experiments measure average performance over 10 episodes, use all \num{18} features introduced in \cref{sec:rethinking-the-state-space} (tile-coded individually), and make use of the physical bandwidth cap rather than pursuing negative reinforcement.
In all cases, agents learn control of the network from scratch.
Test system specifications match those given in \cref{sec:performance-in-an-emulated-environment}.

\begin{table*}
	\centering
	\caption{Average reward for combinations of model, host density and traffic class.\label{tab:av-vals}}
	
	\expandableinput ../tables/big-avg-reward.tex
\end{table*}

\subsection{Congestion-unaware traffic}
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{../plots/udp-2}
	
	\caption{
		Online performance for $n=2$ hosts per egress point when benign traffic is UDP-like.
		Although Marl++ offers a marked improvement (a peak $\sim$\SI{30}{\percent} more benign traffic arrives unimpeded), SPF significantly underperforms for this relatively simple topology.
		Non-SPF agents start off reasonably well, slowly learning better policies.
		\label{fig:udp-2}
	}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{../plots/udp-16}
	
	\caption{
		Online performance for $n=16$ hosts per egress point when benign traffic is UDP-like.
		Marl++ remains marginally ahead of its predecessor, though both have undergone a significant drop in effectiveness.
		SPF, remarkably, displays performance on par with Marl++ for this more difficult topology.
		Both of the new models take longer to train, but achieve better peak and average performance than Marl.
		\label{fig:udp-16}
	}
\end{figure}

Across all choices of $n$, we see that Marl++ exhibits reduced collateral damage compared to Marl, with SPF starting poorly yet becoming more effective for larger $n$ (\cref{tab:av-vals}, \emph{Capped}, UDP).
\Cref{fig:udp-2,fig:udp-16} show the behaviour for $n \in \{2, 16\}$ in detail---the remaining choices of $n$ interpolate between these extremes, with the performance difference between Marl and Marl++ shrinking as $n$ increases.

\subsection{Congestion-aware traffic}
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{../plots/tcp-2}
	
	\caption{
		Online performance for $n=2$ hosts per egress point when benign traffic is TCP-like.
		Marl++ and Marl achieve very similar performance, starting off similarly well without notable improvement over an episode.
		SPF's performance is disappointingly close to baseline, indicating that it is as useful as having no defence system.
		\label{fig:tcp-2}
	}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{../plots/tcp-16}
	
	\caption{
		Online performance for $n=16$ hosts per egress point when benign traffic is UDP-like.
		Although Marl++ takes longer to reach its most effective policy, we achieve visibly better peak performance.
		SPF does manage to learn an improved policy compared to its starting state, but never manages to perform well.
		\label{fig:tcp-16}
	}
\end{figure}

Marl++ consistently allows more legitimate traffic to reach its destination than Marl, with both significantly above the baseline (\cref{tab:av-vals}, \emph{Capped}, TCP).
SPF performs consistently poorly across all choices of $n$.
\Cref{fig:tcp-2}{fig:tcp-16} show two endpoints of a trend in our results: for larger $n$, Marl++ requires around twice as long as Marl to learn its best-achieving policy, underperforming for this period, but achieves notably better peak performance.

\subsection{Banded rewards and negative reinforcement}
In general, we find that explicitly training a policy to keep the system in a valid state via negative reinforcement (i.e., an \emph{uncapped} network) is most effective for UDP-like traffic with Marl++.
For SPF, however, negative reinforcement becomes a worse choice for larger $n$.
Where TCP-like traffic is concerned, we see that this strategy almost always leads to worse policies.

The usefulness of banded rewards with regard to learning an effective policy appears to be unpredictable.
For many of the configurations detailed in \cref{tab:av-vals}, for $n>2$ the average reward with banding is likely to lie between that of the capped and uncapped system models, but oscillates heavily.
In a handful of UDP instances, banding can lead to \emph{better} average performance.
More study is likely required into this; we choose to replace $U_s$ with $0.99U_s$ to minimise the impact on legitimate states, but this choice remains somewhat arbitrary.

\subsection{Single-agent performance}
Making all decisions with a single agent is roughly equivalent to having a zero-cost communication channel between each pair of agents, theoretically allowing faster training by giving each agent more experience.
Curiously, we observe that this often leads to drastically worse policies when used as part of Marl++ for small $n$, but makes SPF a considerably more competitive model---coming ahead of Marl for UDP traffic.
We discuss our conjectures for why this reversal occurs in \cref{sec:discussion}.

\subsection{Per-feature training}

?? Still being performed.

\section{Discussion}\label{sec:discussion}

%Talk about flaws here, what could go wrong...

%?? Why does SPF only do well sometimes? Model is actually more difficult to learn, so it seems to do best when it has a larger set of decisions to learn from. But, it does worse for TCP?
\fakepara{Model performance}
Of the results presented, SPF's meandering performance is interesting given its far smaller action space.
It's natural to expect that this would the model easier to learn, but the additional state required appears to make the model \emph{harder}, beyond even the gains we might expect from choosing a non-zero discount factor.
Accordingly, we see that SPF performs best when agents learn from as much knowledge as possible: high $n$ and single-agent training.
Paradoxically, Marl++ generally achieves the best performance yet actively suffers when treated as a single learner---this may occur due to a roughly even spread of values between disparate actions, due to shared characteristics between legitimate and malicious flows.

%?? May be hard to learn multiple features at once while controlling multiple flows while contending with many more agents, with harder dynamics like TCP. Does this hinder learning in the long run?
Although we have improved upon Marl in both identified problem cases, the improvements are not quite on the order we'd expect, or hope for.
We suspect that some of these difficulties originate from the competitive nature of learning that these models embody: agents are learning action values for multiple features simultaneously, taking many actions at once (making it harder to unpick the true value of each action), and controlling shared global state.
It may be worthwhile to extensively pre-train agents non-competitively on each feature using individual flows, to explore this hypothesis.

%It is likely that the design of Marl++ 

%Finally, it is crucial to note that the models and techniques presented here are an improvement , this work still trails behind existing (exact) DDoS flow detection mechanisms.
Most importantly, what we wish to impart is the knowledge that while the models and techniques we present here are an improvement over past RL-based work, this work still trails behind existing (exact) DDoS flow detection mechanisms.
Although we have conducted work to map the territory to some extent, there are still more advancements to be made before RL-based DDoS defence is truly competitive.

\fakepara{Security concerns and vulnerability}
Can an agent be flooded with new flows to reduce their ability to make decisions?
One of the risks introduced by our timed random sequential policy updates is that so much work can be queued up that an agent is never able to act on some attack flows.
The natural solution is to impose an upper bound on the amount of action computations/policy updates that can be performed before a work list is discarded completely.
This removes the guarantee that all flows will be visited fairly often, but if updates occur regularly then this random sampling my be sufficient to achieve good performance.

%?? Is the state space interpretable? Yes!
Machine learning algorithms have earned a reputation for eluding human interpretation, while being vulnerable to evasion and poisoning.
Given the risks associated with introducing such techniques in the context of security, it is natural to be concerned with the interpretability of the models we have proposed.
With the exception of global state, the tile-coding parameters we make use of ensure that the set of all outputs for each new feature we add is relatively enumerable: $n_{\mathit{tilings}}n_{\mathit{tiles}}\dim{f}$ individual action value vectors per feature $f$, though considerably more combinations thereof.
Furthermore, system state which is dependent on many signals drawn from across a wide network (such as our global state) is difficult to exert precise control over.
These signals' topological separation, in concert with their burstiness and unpredictability, may have substantial effects on an attacker's capabilities.

\section{Related Work}

%?? Try and compare my work here when possible?

\fakepara{DDoS Prevention}
\Textcite{DBLP:conf/lcn/BragaMP10} have examined the detection of ongoing (flooding-based) DDoS attacks through \emph{self-organising maps}, making use of SDN to gather statistics effectively.
Many of their features aren't overly relevant, as their focus is not active defence or discovering \emph{which} hosts are contributing to an attack.

\emph{SPIFFY} \cite{DBLP:conf/ndss/KangGS16} aims to remedy transit-link attacks by observing how flows from each source respond to a sudden increase in available bandwidth.
\Citeauthor{DBLP:conf/ndss/KangGS16} realise that bots participating in an attack are often unable to match this bandwidth expansion due to having already saturated the capacity of their outbound links, while legitimate flows typically speed up to match the new fair-share rate.
%Attackers must either be detected or reduce the throughput of each bot, increasing the cost of launching an attack.
Unlike our approach (and due to the class of attacks it is designed to defend against), SPIFFY is intended to be deployed within ISP networks, although some of our feature choices are backed by similar observations.

\emph{Athena} \cite{DBLP:conf/dsn/LeeKSPY17} is a more generalised SDN framework for intrusion detection, but has shown the use of a \emph{k-nearest neighbours} classifier to detect individual attack flows.
Although heavyweight (and proven to be effective compared with \textcite{DBLP:conf/lcn/BragaMP10}), their comparison against SPIFFY lacks the quantitative evidence required to understand how the system compares.

\Textcite{DBLP:conf/sp/SmithS18} present techniques based on AS-level routing to tackle both transit-link and flooding-based attacks.
This view is taken due to the perceived cost of per-stream classification and inherent sensitivity to adversarial examples or crafted input.
The approach is creative, relying upon BGP \emph{fraudulent route reverse poisoning} to preserve traffic to a target AS, but unlike SPIFFY the approach doesn't actually \emph{remove} the congestion.
Because of this, traditional flooding-based attacks aren't fully alleviated.

%?? Abuses of RL 
\fakepara{RL in Networks}
Earnest, well-considered application of RL towards the challenge of intrusion detection/prevention has seen comparatively little examination.
Past work exhibits treatment of the paradigm as a traditional classifier for anomaly detection \cite{shamshirband2014anomaly} and DDoS prevention \cite{DBLP:conf/mates/ServinK08}.
Given that one of the main strengths of RL techniques is their ability to control ongoing interaction and to adapt by observing the concrete effects of actions taken, such works fail to see how best to apply the rich literature on the subject.

For categorising how RL fits into solving problems, we label works as direct- or indirect-control RL.
A \emph{direct-control} RL problem is one where the RL agent(s) are learning optimal control over a set of actions to act as the \emph{primary} defence or decision-maker---requiring measurements, reward functions and action sets tailored for this purpose.
We feel there is a shortage of work in this category at present, at least in the field of networks; whether this is due to disinterest or difficulty is as-yet unknown.
To date, the best-fitting example we have encountered is that of \textcite{DBLP:journals/eaai/MalialisK15}, which we have covered extensively in \cref{sec:environment-and-rl-algorithm}.

An \emph{indirect-control} RL problem is one where the role played by the agents is to act in service to \emph{another technique} responsible for decision-making, further optimising or generalising aspects of its operation beyond that of hand-coded heuristics.
A past example includes learning when best to \emph{communicate} and share knowledge between \emph{hidden Markov model} anomaly detectors \cite{DBLP:conf/paisi/XuSH07}.
The position of this work is weakened by its reliance on the discredited `DARPA99' dataset \cite{DARPA-IDD, DBLP:conf/cisda/TavallaeeBLG09, DBLP:conf/sp/SommerP10}, but the idea itself is well-treated and this acts as a driver for improvements in this direction.
Outside of anomaly/intrusion detection, there has been growing interest in the use of reinforcement learning in data-driven networking, such as for intra-AS route optimisation \cite{DBLP:conf/hotnets/ValadarskySST17} and for resource-constrained process allocation \cite{DBLP:conf/hotnets/MaoAMK16}.
Work by \textcite{DBLP:conf/sigcomm/MaoNA17} employs client-side observations of network state and video player performance to optimise adaptive bitrate selection for multimedia streaming by RL methods.

\emph{AuTO} \cite{DBLP:conf/sigcomm/ChenL0L18} employs deep RL to perform traffic optimisation, using the \emph{deterministic policy gradients} algorithm \cite{DBLP:conf/icml/SilverLHDWR14}.
Crucially, they find that the vast majority of flows are short-lived, requiring effective decisions in less than a millisecond.
To overcome the high latency of action computation via neural network, two agents are trained, handling aspects of short and long flows respectively.
The first learns to optimise the flow size thresholds which define priority allocation at each switch (and accordingly learn how best to demarcate long and short flows); these short flows are routed by ECMP.
The second agent makes bespoke decisions about routing, prioritisation etc.\ for each of the remaining long flows, who are likely to live long enough that any actions will have a significant impact.

%These works emphasise the necessity of ingenuity in effectively handling how states and actions are represented.

\section{Future Work}

Airlift half of the ``conclusion'' and paste it in here, so that it can be a lot neater.

?? Future Work? I.e., \emph{everything}: no one else is really looking at/interested in this specific kind of application of RL yet. \emph{Yet}.

?? IDEA: try out average reward, TD($\lambda$) methods as future work...

?? What might we do for a reward function in the absence of heuristic estimates and/or explicit a priori knowledge? I think a good candidate is the sum of up and down throughputs (normalised by capacity sum), so long as \emph{neither exceeds the link capacity}. We can extend the team-based formulations similarly. This, in theory, promotes traffic diversity since it's not like flooding-based DDoS attacks are going to submit meaningful work to a server. The intuition, I suppose, is that certain classes of flow will have a small footprint in one direction which causes a sizeable increase in the other! Alternatively, monitor the health of canary flows which cross the team boundary (i.e. only one in-out link).

?? IDEA: Apply these techniques to programmable data planes etc. While it's pretty neat that what we have works assuming that ach router is a software (x86) switch running OVS, what might we need to consider when applying this to `real' switches? ``PDP can allow this to be added to real routers to make it efficient to keep \& process state in the manner we require, as well as enabling more adaptive deployment''. Cite P4, BPFabric, other work on PDPs?

?? Benefit of the more realistic emulation environment is that it is far closer in behaviour and architecture (i.e. viable) to a real SDN-enabled deployment, captures some dynamics which were otherwise hidden/lost by human ignorance. It also allows me to develop the system towards evolving traffic models where it is expected that RL should shine over and above standard ML techniques. THEN: Room to introduce/roll-in dynamic changepoint detection or adaptive exploration \cite{DBLP:conf/ki/Tokic10, DBLP:conf/ki/TokicP11, DBLP:conf/annpr/TokicP12}?

?? What concessions will we have to make in order to make per-flow processing more viable? Intelligent sampling/reanalysis of flows when needed (i.e. an external heuristic guiding method)? In SPIFFY's \cite{DBLP:conf/ndss/KangGS16} evaluation, we see clearly that it can take around \SI{2}{\second} for a flow to react fully to a rate increase---I think for the TCP step it may be wise to factor this in, too!

?? Security? I suspect that the very qualities that make inference difficult in IDS/IPS also increase the level of challenge an advanced threat must overcome.
?? Might want to mention it in related work above, but the recent attention on adversarial examples/tricking models needs to be looked into for RL. Poisoning attacks relevant for online techniques: old bounds exist \textcite{DBLP:journals/jmlr/KloftL10}, new stuff concerns collaborative learners \cite{DBLP:conf/acsac/ShenTS16}, nothing for rl. Hot topic in deep networks \cite{DBLP:conf/eurosp/PapernotMJFCS16, DBLP:conf/eurosp/PapernotMSW18}, but naturally still relevant with even linear approximations or exact tabular case due to limits of the PAC assumption. There is now examination of evasion attacks wrt.\ RL \cite{DBLP:journals/corr/HuangPGDA17}!
?? evasion attacks by \textcite{DBLP:conf/sp/Carlini017}---all of these are computed by way of a general stochastic optimiser, such as \emph{Adam} \cite{DBLP:journals/corr/KingmaB14}. possible to apply something similar to our learned model to assess its security? would the suggested states even be valid? (i.e. since they're monotonically increasing for the most part).

?? More future work --- share knowledge between agents. ``Knowledge bases'' for this purpose? (see: Qianru).

\section{Conclusion}

It all ends here...

?? Please write me once everything else is in place.

?? Overarching goal: works which \emph{respect the complexity of the network environment} (rather than na\"{i}ve simulation, blind ML applications etc.) and choose well-considered pathways to solution. \emph{Call-to-action}?

We stress once again that this work still trails behind existing (exact) DDoS flow detection mechanisms, yet ...

\section*{Acknowledgements}
%Our thanks go to Simon Rogers, Mircea Iordache, Qianru Zhou, Charles Rutherford, Marco Cook and Richard Cziva for their advice, comments and technical assistance.
Our thanks go to Anonymous, Anonymous, Anonymous, Anonymous, Anonymous and Anonymous for their advice, comments and technical assistance.
%Additional thanks \emph{would} go out to my anonymous reviewers, had I any of them.
This work was supported by a nameless funder [grant number 0].

\renewcommand*{\bibfont}{\small}
\printbibliography

\end{document}