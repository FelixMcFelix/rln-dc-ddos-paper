\documentclass[10pt, a4paper]{article}

\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}

\usepackage[binary-units]{siunitx}
\sisetup{range-phrase=--, range-units=single}

\usepackage[basic]{complexity}
\usepackage[super,negative]{nth}

\usepackage{booktabs}
\usepackage[activate={true,nocompatibility},final,tracking=true,kerning=true,spacing=true]{microtype}

%% Fix indent in new section...
\usepackage{titlesec}
\titlespacing*{\section}{0pt}{1.5ex}{0.7ex}

%bib
\usepackage[maxnames=3,maxbibnames=99,mincrossrefs=5,style=ieee,sortcites,backend=bibtex]{biblatex}
\addbibresource{report.bib}

%picky abt et al.
\usepackage{xpatch}

\xpatchbibmacro{name:andothers}{%
	\bibstring{andothers}%
}{%
	\bibstring[\emph]{andothers}%
}{}{}

%opening!

\newcommand{\mytitle}{Lab Report on reimplementation of MARL}

\usepackage{varioref}
\usepackage{hyperref}
\usepackage{url}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black,
	pdftitle={\mytitle{}},
	pdfauthor={Kyle A. Simpson}
}
\usepackage{cleveref}
\newcommand{\crefrangeconjunction}{--}

\newcommand*{\email}[1]{\href{mailto:#1}{\nolinkurl{#1}} } 

\usepackage{titling}
\settowidth{\thanksmarkwidth}{*}
\setlength{\thanksmargin}{-\thanksmarkwidth}

%-------------------------------------%
%-------------------------------------%

\title{\mytitle{}}
\author{Kyle A. Simpson}

\begin{document}

%% If needed, make urls typewritery
%\urlstyle{tt}

\maketitle

\section{Introduction}

Goal: reimplementation of \textcite{DBLP:journals/eaai/MalialisK15}.

\section{Empirical study}

Rough hypotheses:
\begin{itemize}
	\item The findings of \citeauthor{DBLP:journals/eaai/MalialisK15} are replicable in a `real' network environment.
	
	\item Distributed MARL is tested against an abnormal (ISP-like) network where the network administrator is assumed to have control of nodes which are local to few hosts.
	DDoS mitigation via a `direct-control' reinforcement learning approach over blanket pushback will be expected to cause significant collateral damage, and accordingly have significantly lower performance.
	
	\item Learners taught with a gradually/linearly annealed $\epsilon$ in this problem domain will be unable to generalise to the case where host distribution, connection patterns etc.\ exhibit explicit non-stationarity.
	\begin{itemize}
		\item For instance, changepoints at later episodes concerning attacker distribution, host distribution, load profiles...
	\end{itemize}
\end{itemize}

\subsection{Methodology}

\paragraph{Network topology.}
As in the paper:
\begin{itemize}
	\item The topology contains one server node which has a fixed bandwidth $U_s$ to a dedicated switch.
	
	\item The task of DDoS defence is divided between $n$ teams.
	Each team's leader switch connects to the server's switch.
	
	\item Each team then has $m$ intermediate switches, which are connected to it.
	
	\item Every intermediate router has $k$ connected switches which each host a reinforcement learning agent.
	An `external' router is connected to each learner---this is an adaptation to move from the previous ISP-like notion of a topology to be more extensible.
	
	\item Each `external' router then admits $\ell$ hosts.
	Each host is \emph{legitimate} or an \emph{attacker} according to some fixed probability, which governs the volume of bandwidth it sends to the server.
\end{itemize}
This model does not require that the server respond or serve its clients, allowing some simplification: the only flow rules forward packets towards the server indiscriminately.

\paragraph{Traffic generation.}
Traffic is generated using \emph{TCPReplay} at each host, playing back traffic indefinitely from a predefined Pcap file.
??Control over the send rate is managed by either TC, or tcpreplay. I'm having trouble with both at the moment!

The MARL network model requires that hosts be designated `good' or `bad'.
Calculating the reward function for the current network state requires knowledge of which traffic is (il)legitimate, by heuristic or otherwise.
As \citeauthor{DBLP:journals/eaai/MalialisK15} assume perfect knowledge in their training process, I rewrite the traffic from each host to encode this in the last octet of the source IP address.
The scheme is simple: if the octet is even, the packet is `good' (and vice versa).

\paragraph{Load monitoring.}
All switches in Mininet appear to occupy the same namespace, and so a daemon running on any Mininet switch is allowed to monitor the interfaces of any other.
This fact is used to easily monitor each interface in a centralised manner.

The monitoring daemon spawns one thread for each interface it told to subscribe to.
Each of these threads then handles the \emph{libpcap} events attached to that interface, determining whether every received packet is `good' or `bad' and appending the packet length to a dedicated location in a shared store.
Every timestep, the calling program locks the shared store and reads out the time difference, traffic counts, and classifications for each interface.
These statistics enable a rough calculation of the current load across each interface in \si{\mega\bit\per\second}.

\paragraph{Reinforcement learning.} As in the original experiment, I use opportunistic SARSA to learn the state-action value function.
In this variant, the discount factor is removed ($\gamma = 0$), so the model does not encode or learn any longer term planning (i.e.\ take a worse state to eventually reach a more optimal state).

Action selection and learning are simple.
The agent maintains a \emph{value function} $Q(s, a)$, encoding the \emph{value} of taking any action from a given state.
Here, we consider an $\epsilon$-greedy process: a random action is chosen with probability $\epsilon$, otherwise the action with the highest value in the current state is chosen.
This allows for exploration, and is thus linearly annealed.
If exploration is to stop after $t_{end}$ iterations, we write:
\begin{equation}
\epsilon(t) = \epsilon_0(1 - \max[1, t/t_{end}])
\end{equation}
For an action $a_t$ chosen by the current value function $Q$ while in state $s_t$, we observe a \emph{reward} $R_{t+1}$ at the next timestep along with state $s_{t+1}$.
The model may then be updated:
\begin{align}
\begin{split}
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha [R_{t+1} + \gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)] \\
= Q(s_t,a_t) + \alpha [R_{t+1} - Q(s_t,a_t)] 
\end{split}
\end{align}
When discounting is zero as above, SARSA and Q-learning become equivalent.
The learned policy is thus unaware of the action-selection strategy.

%?? Tabular approximation, tile coding, maybe list the params here?!
The current state for a learner is initially a vector containing the bandwidths observed along the route from that learner to the server.
This is discretised using \emph{tile coding}.
The reward $R_t$ for any learner is computed from the proportions of `good' traffic received at the server switch and the team leader, with explicit punishment for saturating the core link.

\paragraph{Pushback.}
Actions received from each learner cause a different level of throttling to be applied within the network---dropping \SIrange{0}{90}{\percent} of traffic forwarded from each learning agent's switch.
This functionality is introduced to Mininet by modification of \emph{Open vSwitch} (OVS).

Mininet's switches run OVS, allowing them to be remotely configured via \emph{OpenFlow} messages.
To enable pushback via probabilistic packet dropping as in the original experiment, the kernel module of OVS was modified to include a custom action to do just this for any given flow.
This action takes the form $\operatorname{PDrop(\mathit{prob})}$, for some integer $\mathit{prob} \in [0, 2^{32}-1]$: for some $x$ uniformly drawn from the same domain, if $x < \mathit{prob}$ then the rest of the actions execute as normal.

Typically, OVS switches register with a single controller: when a packet which can't be routed arrives, the switch forwards this to the controller, which is responsible for replying with new rules to make delivery possible.
The control application acts outside of the typical Switch--Controller relationship: given that rule updates must be periodically posted \emph{at the control application's discretion}, the flow updates are manually composed and sent directly.

\paragraph{Changes for experiments 2... 3...}
Undecided.
Experiment 2 concerns (in theory) adding more and more hosts and seing if the performance ceiling is lowered.
(I.e.\ the bound on how much legit traffic you can get to the server decreases)

Issue w/ experiment 3 is that it's hard to get any meaningful variance if we can't have a large ``max-pool'' of hosts.
I suppose you could keep the total the same, but play about with how the hosts distribute such that there isn't an overload if that always leads to failure anyway.

\subsection{Results}

TBA. Currently awaiting debugging of anomalous mininet behaviour.

\section{Discussion}

Yeah, those sure were some results huh.

\section{Conclusion}

Inconclusive.

\printbibliography

\end{document}